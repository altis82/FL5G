{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "099a94d4-e51d-45a8-abc3-e87bd64a343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we naturally first need to import torch and torchvision\n",
    "import argparse\n",
    "import warnings\n",
    "from collections import OrderedDict\n",
    "\n",
    "from flwr.client import NumPyClient, ClientApp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from tqdm import tqdm\n",
    "NUM_CLIENTS=2\n",
    "BATCH_SIZE=32\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        self.features = dataframe.drop(columns=[\"targetTput\"]).values\n",
    "        self.labels = dataframe[\"targetTput\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.features[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "def load_csv_data(filepath, target_column):\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Identify categorical columns (columns with dtype object)\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    if 'measTimeStampRf' in categorical_columns:\n",
    "        categorical_columns.remove('measTimeStampRf')\n",
    "\n",
    "    # One-hot encode categorical columns\n",
    "    ohe = OneHotEncoder(sparse_output=False, drop='first')\n",
    "    ohe_features = ohe.fit_transform(df[categorical_columns])\n",
    "    ohe_feature_names = ohe.get_feature_names_out(categorical_columns)\n",
    "\n",
    "    # Create a new DataFrame with one-hot encoded features\n",
    "    ohe_df = pd.DataFrame(ohe_features, columns=ohe_feature_names)\n",
    "\n",
    "    # Drop original categorical columns and 'measTimeStampRf', then concatenate with the one-hot encoded features\n",
    "    df.drop(columns=categorical_columns + ['measTimeStampRf'], inplace=True)\n",
    "    df = pd.concat([df.reset_index(drop=True), ohe_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # Split dataset into train, validation, and test sets\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Split train set into NUM_CLIENTS partitions\n",
    "    train_partitions = np.array_split(train_df, NUM_CLIENTS)\n",
    "\n",
    "    trainloaders = [DataLoader(CustomDataset(partition), batch_size=BATCH_SIZE, shuffle=True) for partition in train_partitions]\n",
    "    testloader = DataLoader(CustomDataset(test_df), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    return trainloaders, testloader, train_df.shape[1] - 1  # Subtract 1 to exclude the target column from input dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4053e058-1ef6-49ac-8f9e-559f202379e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension: 2\n",
      "\n",
      "Showing data from the first train loader:\n",
      "Batch 1\n",
      "Inputs:\n",
      "tensor([[1.0130e+03, 4.5000e+01, 1.6401e-01,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [1.0130e+03, 6.8000e+01, 3.0000e-01,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [1.0050e+03, 9.1000e+01, 3.5224e-01,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [1.0030e+03, 5.6000e+01, 1.0000e-01,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+00],\n",
      "        [1.0060e+03, 6.2000e+01, 2.5000e-01,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [1.0050e+03, 9.1000e+01, 3.7300e-01,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00]])\n",
      "Targets:\n",
      "tensor([0.3000, 0.3000, 0.7500, 0.7500, 0.2500, 0.2500, 0.7500, 0.1000, 0.1000,\n",
      "        0.7500, 0.3000, 0.1000, 0.2500, 0.1000, 0.1000, 0.2500, 0.1000, 0.7500,\n",
      "        0.1000, 0.1000, 0.1000, 0.1000, 0.3000, 0.1000, 0.7500, 0.7500, 0.3000,\n",
      "        0.1000, 0.1000, 0.1000, 0.2500, 0.7500])\n",
      "batch: [tensor([[1.0090e+03, 1.3600e+02, 8.5039e-03,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [1.0030e+03, 4.5000e+01, 7.6249e-02,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [1.0050e+03, 9.1000e+01, 3.9087e-01,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [1.0030e+03, 4.5000e+01, 7.6249e-02,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [1.0020e+03, 9.1000e+01, 1.5926e-01,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [1.0130e+03, 6.8000e+01, 3.0000e-01,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00]]), tensor([0.1000, 0.1000, 0.7500, 0.7500, 0.7500, 0.1000, 0.1000, 0.7500, 0.1000,\n",
      "        0.2500, 0.3000, 0.2500, 0.3000, 0.3000, 0.2500, 0.3000, 0.3000, 0.3000,\n",
      "        0.1000, 0.1000, 0.3000, 0.7500, 0.1000, 0.1000, 0.7500, 0.1000, 0.3000,\n",
      "        0.1000, 0.2500, 0.1000, 0.7500, 0.3000])] 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/anaconda3/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "def show_data_from_loader(loader, num_batches=1):\n",
    "    for i, (inputs, targets) in enumerate(loader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        print(f\"Batch {i + 1}\")\n",
    "        print(\"Inputs:\")\n",
    "        print(inputs)\n",
    "        print(\"Targets:\")\n",
    "        print(targets)\n",
    "\n",
    "# Example usage:\n",
    "trainloaders, testloader, input_dim = load_csv_data('src_ue.csv','targetTput')\n",
    "#Show dimension of trainloaders\n",
    "print(f\"Dimension: {len(trainloaders)}\")\n",
    "\n",
    "# Show data from the first train loader\n",
    "print(\"\\nShowing data from the first train loader:\")\n",
    "show_data_from_loader(trainloaders[0])\n",
    "# Inspect the data from the DataLoader\n",
    "for batch in trainloaders[0]:\n",
    "    print(\"batch:\",batch,len(batch))\n",
    "    break  # Remove break to see more examples if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ee65d5-eb44-4ad7-90b8-b28242a7ee02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0155ba40-e93b-4eb5-813c-71f5fa2c6012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_parameters = 24705\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cpu\")  # Try \"cuda\" to train on GPU\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "input_size = next(iter(trainloaders[0]))[0].shape[1]\n",
    "\n",
    "# Initialize the network\n",
    "net = Net(input_size).to(DEVICE)\n",
    "\n",
    "num_parameters = sum(value.numel() for value in net.state_dict().values())\n",
    "print(f\"{num_parameters = }\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "07beb5a8-7f74-4e11-ac5f-0d5b99fc2a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainloaders, epochs: int, verbose=False):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for trainloader in trainloaders:  # Iterate over each client's data\n",
    "            for features, targets in trainloader:\n",
    "                features, targets = features.to(DEVICE), targets.to(DEVICE)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(features)\n",
    "                loss = criterion(outputs, targets.view(-1, 1))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "        running_loss /= len(trainloaders) * len(trainloader.dataset)\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}: train loss {running_loss}\")\n",
    "    return net\n",
    "\n",
    "def test(net, testloader):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    total_loss = 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for features, targets in testloader:\n",
    "            features, targets = features.to(DEVICE), targets.to(DEVICE)\n",
    "            outputs = net(features)\n",
    "            loss = criterion(outputs, targets.view(-1, 1))\n",
    "            total_loss += loss.item()\n",
    "    total_loss /= len(testloader.dataset)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "bf078510-e2d0-4d5f-8fc2-0b33ba4b60a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_centralised(epochs: int, lr: float, momentum: float = 0.9):\n",
    "    \"\"\"A minimal (but complete) training loop\"\"\"\n",
    "\n",
    "    # instantiate the model\n",
    "    model = Net(input_size).to(DEVICE)\n",
    "\n",
    "    # define optimiser with hyperparameters supplied\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "    # get dataset and construct a dataloaders\n",
    "    trainset, testset, input_dim = load_csv_data('src_ue.csv','targetTput')\n",
    "    #trainloader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "    testloader = DataLoader(testset, batch_size=32)\n",
    "\n",
    "    # train for the specified number of epochs\n",
    "    trained_model = train(model, trainset, epochs)\n",
    "\n",
    "    # training is completed, then evaluate model on the test set\n",
    "    loss = test(trained_model, testset)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f25570a7-8e36-4d17-9678-e2d2e1cf4ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/anaconda3/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.00012120244570542127\n"
     ]
    }
   ],
   "source": [
    "run_centralised(epochs=5, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0eec0b53-a38f-4985-9c82-4203d0ccdfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/anaconda3/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 0.0115\n",
      "Epoch 2: train loss 0.0002\n",
      "Epoch 3: train loss 0.0002\n",
      "Epoch 4: train loss 0.0001\n",
      "Epoch 5: train loss 0.0001\n",
      "Epoch 6: train loss 0.0000\n",
      "Epoch 7: train loss 0.0001\n",
      "Epoch 8: train loss 0.0001\n",
      "Epoch 9: train loss 0.0000\n",
      "Epoch 10: train loss 0.0000\n",
      "Test Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")  # Change to \"cuda\" for GPU support\n",
    "NUM_CLIENTS = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "        self.feature_columns = [col for col in dataframe.columns if col != 'targetTput' and col != 'measTimeStampRf']\n",
    "        self.target_column = 'targetTput'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        features = torch.tensor(row[self.feature_columns].values.astype(np.float32), dtype=torch.float32)\n",
    "        target = torch.tensor(row[self.target_column].astype(np.float32), dtype=torch.float32)\n",
    "        return features, target\n",
    "def load_csv_dataset(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Identify categorical columns\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    # Remove 'measTimeStampRf' from categorical columns as it's not relevant for one-hot encoding\n",
    "    if 'measTimeStampRf' in categorical_columns:\n",
    "        categorical_columns.remove('measTimeStampRf')\n",
    "\n",
    "    # One-hot encode categorical columns\n",
    "    ohe = OneHotEncoder(sparse_output=False, drop='first')  # Updated parameter name\n",
    "    ohe_features = ohe.fit_transform(df[categorical_columns])\n",
    "    ohe_feature_names = ohe.get_feature_names_out(categorical_columns)\n",
    "\n",
    "    # Create a DataFrame with one-hot encoded features\n",
    "    ohe_df = pd.DataFrame(ohe_features, columns=ohe_feature_names)\n",
    "\n",
    "    # Drop original categorical columns and 'measTimeStampRf'\n",
    "    df.drop(columns=categorical_columns + ['measTimeStampRf'], inplace=True)\n",
    "    df = pd.concat([df.reset_index(drop=True), ohe_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # Split dataset\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Split train set into NUM_CLIENTS partitions\n",
    "    train_partitions = np.array_split(train_df, NUM_CLIENTS)\n",
    "\n",
    "    trainloaders = [DataLoader(CustomDataset(partition), batch_size=BATCH_SIZE, shuffle=True) for partition in train_partitions]\n",
    "    testloader = DataLoader(CustomDataset(test_df), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    return trainloaders, testloader, len(train_df.columns) - 1  # Subtract 1 for the target column\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size: int) -> None:\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "def train(net, trainloaders, epochs: int, verbose=False):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(net.parameters())\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for trainloader in trainloaders:  # Iterate over each client's data\n",
    "            for features, targets in trainloader:\n",
    "                features, targets = features.to(DEVICE), targets.to(DEVICE)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(features)\n",
    "                loss = criterion(outputs, targets.view(-1, 1))  # Ensure targets have the correct shape\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "        average_loss = running_loss / (len(trainloaders) * len(trainloader.dataset))\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}: train loss {average_loss:.4f}\")\n",
    "\n",
    "def test(net, testloader):\n",
    "    \"\"\"Evaluate the network on the test set.\"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    total_loss = 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for features, targets in testloader:\n",
    "            features, targets = features.to(DEVICE), targets.to(DEVICE)\n",
    "            outputs = net(features)\n",
    "            loss = criterion(outputs, targets.view(-1, 1))  # Ensure targets have the correct shape\n",
    "            total_loss += loss.item()\n",
    "    average_loss = total_loss / len(testloader.dataset)\n",
    "    return average_loss\n",
    "\n",
    "def run_centralized(epochs: int, lr: float, batch_size: int):\n",
    "    \"\"\"Perform centralized training and evaluation.\"\"\"\n",
    "    # Load dataset and construct DataLoaders\n",
    "    trainloaders, testloader, input_size = load_csv_dataset('src_ue.csv')\n",
    "\n",
    "    # Initialize the network\n",
    "    net = Net(input_size).to(DEVICE)\n",
    "\n",
    "    # Train the model\n",
    "    train(net, trainloaders, epochs=epochs, verbose=True)\n",
    "\n",
    "    # Test the model\n",
    "    test_loss = test(net, testloader)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Example usage\n",
    "run_centralized(epochs=10, lr=0.001, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "be156c53-f323-41b8-aec7-b429003da933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import torch\n",
    "from flwr.common import NDArrays, Scalar\n",
    "\n",
    "trainloaders, testloader, input_size = load_csv_dataset('src_ue.csv')\n",
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, trainloader, valloader,input_size) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size= input_size\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "        self.model = Net(input_size).to(DEVICE)\n",
    "\n",
    "    def set_parameters(self, parameters):\n",
    "        \"\"\"With the model parameters received from the server,\n",
    "        overwrite the uninitialise model in this class with them.\"\"\"\n",
    "\n",
    "        params_dict = zip(self.model.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "        # now replace the parameters\n",
    "        self.model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    def get_parameters(self, config: Dict[str, Scalar]):\n",
    "        \"\"\"Extract all model parameters and convert them to a list of\n",
    "        NumPy arrays. The server doesn't work with PyTorch/TF/etc.\"\"\"\n",
    "        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        \"\"\"This method train the model using the parameters sent by the\n",
    "        server on the dataset of this client. At then end, the parameters\n",
    "        of the locally trained model are communicated back to the server\"\"\"\n",
    "\n",
    "        # copy parameters sent by the server into client's local model\n",
    "        self.set_parameters(parameters)\n",
    "\n",
    "        # Define the optimizer -------------------------------------------------------------- Essentially the same as in the centralised example above\n",
    "        optim = torch.optim.SGD(self.model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "        # do local training  -------------------------------------------------------------- Essentially the same as in the centralised example above (but now using the client's data instead of the whole dataset)\n",
    "        train(self.model, self.trainloader, optim, epochs=1)\n",
    "\n",
    "        # return the model parameters to the server as well as extra info (number of training examples in this case)\n",
    "        return self.get_parameters({}), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters: NDArrays, config: Dict[str, Scalar]):\n",
    "        \"\"\"Evaluate the model sent by the server on this client's\n",
    "        local validation set. Then return performance metrics.\"\"\"\n",
    "\n",
    "        self.set_parameters(parameters)\n",
    "        loss = test(\n",
    "            self.model, self.valloader\n",
    "        )  # <-------------------------- calls the `test` function, just what we did in the centralised setting (but this time using the client's local validation set)\n",
    "        # send statistics back to the server\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": accuracy}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ae4e6ffc-54d6-4c02-b075-8927ab1a1876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluate_fn(testloader):\n",
    "    \"\"\"This is a function that returns a function. The returned\n",
    "    function (i.e. `evaluate_fn`) will be executed by the strategy\n",
    "    at the end of each round to evaluate the stat of the global\n",
    "    model.\"\"\"\n",
    "\n",
    "    def evaluate_fn(server_round: int, parameters, config):\n",
    "        \"\"\"This function is executed by the strategy it will instantiate\n",
    "        a model and replace its parameters with those from the global model.\n",
    "        The, the model will be evaluate on the test set (recall this is the\n",
    "        whole MNIST test set).\"\"\"\n",
    "\n",
    "        model = Net(input_size).to(DEVICE)\n",
    "\n",
    "        # set parameters to the model\n",
    "        params_dict = zip(model.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "        model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "        # call test\n",
    "        loss, accuracy = test(\n",
    "            model, testloader\n",
    "        )  # <-------------------------- calls the `test` function, just what we did in the centralised setting\n",
    "        return loss, {\"accuracy\": accuracy}\n",
    "\n",
    "    return evaluate_fn\n",
    "\n",
    "\n",
    "# now we can define the strategy\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "    fraction_fit=0.1,  # let's sample 10% of the client each round to do local training\n",
    "    fraction_evaluate=0.1,  # after each round, let's sample 20% of the clients to asses how well the global model is doing\n",
    "    min_available_clients=100,  # total number of clients available in the experiment\n",
    "    evaluate_fn=get_evaluate_fn(testloader),\n",
    ")  # a callback to a function that the strategy can execute to evaluate the state of the global model on a centralised dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1145388e-62d0-4bd5-bbcd-7fdd592d1c57",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (20893064.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[165], line 7\u001b[0;36m\u001b[0m\n\u001b[0;31m    ).to_client()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "def generate_client_fn(trainloaders, valloaders,input_size):\n",
    "    def client_fn(cid: str):\n",
    "        \"\"\"Returns a FlowerClient containing the cid-th data partition\"\"\"\n",
    "\n",
    "        return FlowerClient(\n",
    "            trainloader=trainloaders[int(cid)], valloader=valloaders[int(cid)], input_size\n",
    "        ).to_client()\n",
    "\n",
    "    return client_fn\n",
    "\n",
    "\n",
    "client_fn_callback = generate_client_fn(trainloaders, valloaders,input_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
